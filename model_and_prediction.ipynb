{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPs8zEE4VSUwL+25gKASHEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z-yoscovits/Unfair_tos/blob/main/model_and_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdZaMP25KWIB",
        "outputId": "022b97a6-e878-400a-989d-d2ed74a8295b"
      },
      "source": [
        "%pip install -U gensim\n",
        "%pip install -U imbalanced-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 168kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n",
            "Collecting imbalanced-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/98/dc784205a7e3034e84d41ac4781660c67ad6327f2f5a80c568df31673d1c/imbalanced_learn-0.8.0-py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn, imbalanced-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "Successfully installed imbalanced-learn-0.8.0 scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SyJfdR_KaWB",
        "outputId": "89097d72-50be-4a3e-d17e-39b075c38d43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwHFJc7OKd3y"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrrS-zb6k6Ot"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD4U5f9ZKoxJ"
      },
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/data_challenge/Terms_of_service - terms_of_service.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmE2RBmDKpI8"
      },
      "source": [
        "#Remove sentences that have both labels\n",
        "counts=data.drop_duplicates().sentence1.value_counts()\n",
        "contradiction=list(counts[counts>1].index)\n",
        "data=data[~data.sentence1.isin(contradiction)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUfKNxyJKsVU",
        "outputId": "676cf805-0d0c-495e-e2c5-9df0f27ed205"
      },
      "source": [
        "# There are two Sentences with both labels\n",
        "contradiction"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"if you repeatedly infringe other people 's intellectual property rights , we will disable your account when appropriate .\",\n",
              " '#NAME?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edOyYs6DKuZe",
        "outputId": "3a777f0a-efae-4b3c-aaec-4939cb10f6dd"
      },
      "source": [
        "data.label.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    8363\n",
              "1    1030\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCrvqWJzKzTQ",
        "outputId": "d41ea821-12aa-4a70-fb3f-d9ab9e99e55f"
      },
      "source": [
        "data.drop_duplicates().label.value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    8198\n",
              "1    1014\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDZUZYHLK2HP"
      },
      "source": [
        "#There are a handfull of duplicate sentences, removing them to prevent data leakage (i.e the same sentence in both train and test sets)\n",
        "data=data.drop_duplicates()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl9IR0XeK3-v"
      },
      "source": [
        "#Create val and dev sets\n",
        "dev_data, val_data=train_test_split(data, test_size=0.1, random_state=42)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUvr-WQjK6r2"
      },
      "source": [
        "#Split dev set into train and test sets, X is kept as a single column dataframe, rather than a Series because that is what imbalanced-learn expects\n",
        "X_train, X_test, y_train, y_test =train_test_split(dev_data[['sentence1']], dev_data.label, test_size=0.2, random_state=42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ1-7PqLK8hW"
      },
      "source": [
        "#Over sample minority class to produce balanced training set, leave test set and val set unbalanced so metrics are fair\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils import shuffle\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "X_train,y_train=shuffle(X_resampled, y_resampled)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A0B17JwK-NH"
      },
      "source": [
        "#Turn 1-column dataframes back into series\n",
        "X_train=X_train.sentence1\n",
        "X_test=X_test.sentence1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0FZEH3bK_2W",
        "outputId": "051c7c6d-b09d-4141-bfc7-d59594ea9aa6"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JWx9Q-fLBm9"
      },
      "source": [
        "#Load custom trained word2vec\n",
        "w2v = gensim.models.Word2Vec.load('/content/drive/MyDrive/data_challenge/word2vec.model')\n",
        "vocab = w2v.wv.index_to_key  \n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvM_5FilLD0B"
      },
      "source": [
        "vocab_size = len(vocab) + 1\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-i7oNl_LHkW"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, AveragePooling1D, Dropout, LSTM, Bidirectional, Embedding, Concatenate, Reshape, Subtract, Dot\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ia9bicTLJQi"
      },
      "source": [
        "from gensim import utils"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgjHNaUOLLKd"
      },
      "source": [
        "def prepare_inputs(x_sentences, sequence_length):\n",
        "  x_preprocessed=x_sentences.map(utils.simple_preprocess)\n",
        "  x_encoded=tokenizer.texts_to_sequences(x_preprocessed)\n",
        "  x_input = np.asarray(pad_sequences(x_encoded,maxlen=sequence_length, padding='pre'))\n",
        "  return x_input\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efKWtbw3LMqH"
      },
      "source": [
        "max_len=75\n",
        "\n",
        "input_train = prepare_inputs(X_train, max_len)\n",
        "input_test = prepare_inputs(X_test, max_len)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V1SExscLOK_"
      },
      "source": [
        "#Initialize embedding layer with word2vec model\n",
        "import numpy as np\n",
        "\n",
        "def get_weight_matrix():\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, w2v.vector_size))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for i in range(len(vocab)):\n",
        "        weight_matrix[i + 1] = w2v.wv.get_vector(vocab[i], norm=True)\n",
        "    return weight_matrix\n",
        "\n",
        "embedding_vectors = get_weight_matrix()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMKkitkLP5U"
      },
      "source": [
        "tf.keras.backend.clear_session() \n",
        "emb_layer = Embedding(vocab_size, output_dim=w2v.vector_size, weights=[embedding_vectors], input_length=max_len, trainable=True)\n",
        "\n",
        "sentence_input=Input(shape=(max_len,))\n",
        "sentence_embedding=emb_layer(sentence_input)\n",
        "sentence_embedding=Dropout(0.9)(sentence_embedding)\n",
        "sentence_lstm=Bidirectional(LSTM(16))(sentence_embedding)\n",
        "x=Dropout(0.7)(sentence_lstm)\n",
        "x=Dense(16, activation='relu')(x)\n",
        "x=Dropout(0.7)(x)\n",
        "output=Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(sentence_input,output)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYig1Wi4LWy0",
        "outputId": "991137dc-a1d1-4c7f-baf9-b1f1c3d94f83"
      },
      "source": [
        "epochs=10\n",
        "batch_size=512\n",
        "model.fit(input_train, y_train, epochs=epochs, batch_size=batch_size,  validation_data=(input_test, y_test))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "24/24 [==============================] - 10s 318ms/step - loss: 0.7052 - precision: 0.5056 - recall: 0.5071 - val_loss: 0.6900 - val_precision: 0.1041 - val_recall: 0.3242\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 7s 286ms/step - loss: 0.7054 - precision: 0.4968 - recall: 0.5178 - val_loss: 0.6923 - val_precision: 0.1047 - val_recall: 0.4286\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.7030 - precision: 0.5034 - recall: 0.5376 - val_loss: 0.6939 - val_precision: 0.1111 - val_recall: 0.5440\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6988 - precision: 0.5098 - recall: 0.5477 - val_loss: 0.6952 - val_precision: 0.1135 - val_recall: 0.6484\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.7042 - precision: 0.4922 - recall: 0.5416 - val_loss: 0.6961 - val_precision: 0.1142 - val_recall: 0.7363\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.7026 - precision: 0.4981 - recall: 0.5621 - val_loss: 0.6961 - val_precision: 0.1187 - val_recall: 0.7857\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 7s 286ms/step - loss: 0.7002 - precision: 0.5036 - recall: 0.5787 - val_loss: 0.6968 - val_precision: 0.1182 - val_recall: 0.8407\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.7001 - precision: 0.5047 - recall: 0.5816 - val_loss: 0.6969 - val_precision: 0.1180 - val_recall: 0.8626\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.7001 - precision: 0.4979 - recall: 0.5897 - val_loss: 0.6978 - val_precision: 0.1138 - val_recall: 0.8901\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 7s 287ms/step - loss: 0.6969 - precision: 0.5083 - recall: 0.5970 - val_loss: 0.6986 - val_precision: 0.1122 - val_recall: 0.9286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efb08f1fa10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhL96_oQMLDk",
        "outputId": "05821bff-c64e-47ce-fe82-6e8908e9a22c"
      },
      "source": [
        "epochs=100\n",
        "batch_size=512\n",
        "model.fit(input_train, y_train, epochs=epochs, batch_size=batch_size,  validation_data=(input_test, y_test))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/24 [==============================] - 7s 287ms/step - loss: 0.6973 - precision: 0.5060 - recall: 0.6032 - val_loss: 0.6993 - val_precision: 0.1123 - val_recall: 0.9615\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6974 - precision: 0.5103 - recall: 0.6229 - val_loss: 0.6996 - val_precision: 0.1110 - val_recall: 0.9670\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6957 - precision: 0.5067 - recall: 0.6180 - val_loss: 0.6997 - val_precision: 0.1110 - val_recall: 0.9780\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6967 - precision: 0.5054 - recall: 0.6195 - val_loss: 0.6997 - val_precision: 0.1118 - val_recall: 0.9890\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6962 - precision: 0.5011 - recall: 0.6161 - val_loss: 0.6999 - val_precision: 0.1124 - val_recall: 1.0000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6967 - precision: 0.5054 - recall: 0.6300 - val_loss: 0.6999 - val_precision: 0.1122 - val_recall: 1.0000\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6966 - precision: 0.5054 - recall: 0.6244 - val_loss: 0.6999 - val_precision: 0.1118 - val_recall: 1.0000\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6954 - precision: 0.5071 - recall: 0.6295 - val_loss: 0.6996 - val_precision: 0.1117 - val_recall: 1.0000\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6963 - precision: 0.5054 - recall: 0.6359 - val_loss: 0.6997 - val_precision: 0.1115 - val_recall: 1.0000\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6935 - precision: 0.5181 - recall: 0.6474 - val_loss: 0.6998 - val_precision: 0.1112 - val_recall: 1.0000\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6934 - precision: 0.5147 - recall: 0.6508 - val_loss: 0.6998 - val_precision: 0.1111 - val_recall: 1.0000\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6943 - precision: 0.5085 - recall: 0.6481 - val_loss: 0.6999 - val_precision: 0.1110 - val_recall: 1.0000\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6930 - precision: 0.5089 - recall: 0.6552 - val_loss: 0.7002 - val_precision: 0.1106 - val_recall: 1.0000\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6935 - precision: 0.5107 - recall: 0.6542 - val_loss: 0.7000 - val_precision: 0.1107 - val_recall: 1.0000\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6937 - precision: 0.5085 - recall: 0.6407 - val_loss: 0.7002 - val_precision: 0.1106 - val_recall: 1.0000\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6927 - precision: 0.5115 - recall: 0.6527 - val_loss: 0.7003 - val_precision: 0.1104 - val_recall: 1.0000\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6928 - precision: 0.5109 - recall: 0.6601 - val_loss: 0.7005 - val_precision: 0.1105 - val_recall: 1.0000\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6924 - precision: 0.5117 - recall: 0.6517 - val_loss: 0.7003 - val_precision: 0.1107 - val_recall: 1.0000\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6923 - precision: 0.5133 - recall: 0.6682 - val_loss: 0.7001 - val_precision: 0.1107 - val_recall: 1.0000\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6934 - precision: 0.5053 - recall: 0.6544 - val_loss: 0.7000 - val_precision: 0.1108 - val_recall: 1.0000\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6927 - precision: 0.5122 - recall: 0.6552 - val_loss: 0.7003 - val_precision: 0.1109 - val_recall: 1.0000\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 7s 279ms/step - loss: 0.6932 - precision: 0.5119 - recall: 0.6557 - val_loss: 0.7008 - val_precision: 0.1108 - val_recall: 1.0000\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6924 - precision: 0.5172 - recall: 0.6672 - val_loss: 0.7013 - val_precision: 0.1106 - val_recall: 1.0000\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 7s 286ms/step - loss: 0.6913 - precision: 0.5099 - recall: 0.6578 - val_loss: 0.7014 - val_precision: 0.1108 - val_recall: 1.0000\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6919 - precision: 0.5113 - recall: 0.6606 - val_loss: 0.7017 - val_precision: 0.1110 - val_recall: 1.0000\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6908 - precision: 0.5096 - recall: 0.6627 - val_loss: 0.7019 - val_precision: 0.1112 - val_recall: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6883 - precision: 0.5160 - recall: 0.6642 - val_loss: 0.7024 - val_precision: 0.1118 - val_recall: 1.0000\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.6892 - precision: 0.5177 - recall: 0.6733 - val_loss: 0.7031 - val_precision: 0.1120 - val_recall: 1.0000\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6900 - precision: 0.5188 - recall: 0.6826 - val_loss: 0.7039 - val_precision: 0.1121 - val_recall: 1.0000\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6886 - precision: 0.5145 - recall: 0.6743 - val_loss: 0.7058 - val_precision: 0.1126 - val_recall: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6876 - precision: 0.5191 - recall: 0.6926 - val_loss: 0.7094 - val_precision: 0.1131 - val_recall: 1.0000\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6853 - precision: 0.5253 - recall: 0.7089 - val_loss: 0.7133 - val_precision: 0.1141 - val_recall: 1.0000\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.6877 - precision: 0.5201 - recall: 0.7016 - val_loss: 0.7154 - val_precision: 0.1175 - val_recall: 1.0000\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 7s 279ms/step - loss: 0.6882 - precision: 0.5265 - recall: 0.6921 - val_loss: 0.7166 - val_precision: 0.1217 - val_recall: 1.0000\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6854 - precision: 0.5296 - recall: 0.6925 - val_loss: 0.7150 - val_precision: 0.1265 - val_recall: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 7s 286ms/step - loss: 0.6831 - precision: 0.5404 - recall: 0.6510 - val_loss: 0.7168 - val_precision: 0.1378 - val_recall: 0.9505\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6824 - precision: 0.5487 - recall: 0.5438 - val_loss: 0.7244 - val_precision: 0.1451 - val_recall: 0.9615\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6809 - precision: 0.5558 - recall: 0.5594 - val_loss: 0.7212 - val_precision: 0.1616 - val_recall: 0.9396\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6822 - precision: 0.5504 - recall: 0.5240 - val_loss: 0.7276 - val_precision: 0.1576 - val_recall: 0.9286\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6818 - precision: 0.5571 - recall: 0.5420 - val_loss: 0.7240 - val_precision: 0.1699 - val_recall: 0.9121\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6808 - precision: 0.5597 - recall: 0.5433 - val_loss: 0.7242 - val_precision: 0.1733 - val_recall: 0.9066\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6774 - precision: 0.5704 - recall: 0.5560 - val_loss: 0.7396 - val_precision: 0.1553 - val_recall: 0.9505\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.6753 - precision: 0.5697 - recall: 0.5878 - val_loss: 0.7086 - val_precision: 0.2051 - val_recall: 0.7912\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6731 - precision: 0.5858 - recall: 0.5631 - val_loss: 0.7401 - val_precision: 0.1637 - val_recall: 0.9451\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6735 - precision: 0.5830 - recall: 0.5361 - val_loss: 0.7336 - val_precision: 0.1677 - val_recall: 0.9231\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6692 - precision: 0.5877 - recall: 0.5643 - val_loss: 0.7416 - val_precision: 0.1672 - val_recall: 0.9451\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.6664 - precision: 0.6029 - recall: 0.5477 - val_loss: 0.7501 - val_precision: 0.1675 - val_recall: 0.9505\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6575 - precision: 0.6105 - recall: 0.6078 - val_loss: 0.7452 - val_precision: 0.1757 - val_recall: 0.9451\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6523 - precision: 0.6258 - recall: 0.5831 - val_loss: 0.7253 - val_precision: 0.1950 - val_recall: 0.9396\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.6465 - precision: 0.6338 - recall: 0.6038 - val_loss: 0.7363 - val_precision: 0.1880 - val_recall: 0.9451\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6463 - precision: 0.6384 - recall: 0.5985 - val_loss: 0.7015 - val_precision: 0.2169 - val_recall: 0.9451\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6401 - precision: 0.6487 - recall: 0.5939 - val_loss: 0.6970 - val_precision: 0.2211 - val_recall: 0.9451\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6374 - precision: 0.6698 - recall: 0.5960 - val_loss: 0.6617 - val_precision: 0.2562 - val_recall: 0.9011\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.6264 - precision: 0.6753 - recall: 0.6097 - val_loss: 0.6598 - val_precision: 0.2551 - val_recall: 0.9011\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6219 - precision: 0.6799 - recall: 0.6278 - val_loss: 0.6531 - val_precision: 0.2615 - val_recall: 0.9066\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.6229 - precision: 0.6893 - recall: 0.6071 - val_loss: 0.6575 - val_precision: 0.2541 - val_recall: 0.9396\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6217 - precision: 0.6956 - recall: 0.6104 - val_loss: 0.6611 - val_precision: 0.2507 - val_recall: 0.9505\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6100 - precision: 0.7033 - recall: 0.6403 - val_loss: 0.6148 - val_precision: 0.2862 - val_recall: 0.9011\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.6078 - precision: 0.7051 - recall: 0.6518 - val_loss: 0.5707 - val_precision: 0.3243 - val_recall: 0.8516\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5998 - precision: 0.7241 - recall: 0.6400 - val_loss: 0.5966 - val_precision: 0.2813 - val_recall: 0.9011\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5861 - precision: 0.7372 - recall: 0.6647 - val_loss: 0.5877 - val_precision: 0.2757 - val_recall: 0.9121\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5884 - precision: 0.7311 - recall: 0.6527 - val_loss: 0.5691 - val_precision: 0.2847 - val_recall: 0.9121\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.5780 - precision: 0.7485 - recall: 0.6689 - val_loss: 0.5779 - val_precision: 0.2733 - val_recall: 0.9176\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5706 - precision: 0.7505 - recall: 0.6877 - val_loss: 0.5475 - val_precision: 0.2908 - val_recall: 0.9011\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.5635 - precision: 0.7612 - recall: 0.6891 - val_loss: 0.5317 - val_precision: 0.2959 - val_recall: 0.9121\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.5557 - precision: 0.7621 - recall: 0.6969 - val_loss: 0.5373 - val_precision: 0.2850 - val_recall: 0.9286\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.5582 - precision: 0.7623 - recall: 0.6923 - val_loss: 0.4843 - val_precision: 0.3299 - val_recall: 0.8791\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.5522 - precision: 0.7739 - recall: 0.6945 - val_loss: 0.4892 - val_precision: 0.3221 - val_recall: 0.8901\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5534 - precision: 0.7663 - recall: 0.7050 - val_loss: 0.5298 - val_precision: 0.2876 - val_recall: 0.9341\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.5428 - precision: 0.7698 - recall: 0.7194 - val_loss: 0.5079 - val_precision: 0.2982 - val_recall: 0.9176\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.5436 - precision: 0.7752 - recall: 0.7227 - val_loss: 0.4496 - val_precision: 0.3594 - val_recall: 0.8846\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.5359 - precision: 0.7836 - recall: 0.7158 - val_loss: 0.4917 - val_precision: 0.3156 - val_recall: 0.9121\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5322 - precision: 0.7905 - recall: 0.7338 - val_loss: 0.4659 - val_precision: 0.3382 - val_recall: 0.8956\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.5212 - precision: 0.7901 - recall: 0.7358 - val_loss: 0.4250 - val_precision: 0.3831 - val_recall: 0.8736\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.5292 - precision: 0.7874 - recall: 0.7324 - val_loss: 0.4172 - val_precision: 0.3869 - val_recall: 0.8736\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.5149 - precision: 0.7981 - recall: 0.7295 - val_loss: 0.4368 - val_precision: 0.3628 - val_recall: 0.9011\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.5138 - precision: 0.7915 - recall: 0.7382 - val_loss: 0.4168 - val_precision: 0.3750 - val_recall: 0.8901\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.5122 - precision: 0.8041 - recall: 0.7387 - val_loss: 0.4289 - val_precision: 0.3741 - val_recall: 0.9066\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.5053 - precision: 0.7980 - recall: 0.7461 - val_loss: 0.4111 - val_precision: 0.3817 - val_recall: 0.8956\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4958 - precision: 0.8055 - recall: 0.7498 - val_loss: 0.4264 - val_precision: 0.3604 - val_recall: 0.9011\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4941 - precision: 0.8070 - recall: 0.7525 - val_loss: 0.3843 - val_precision: 0.4046 - val_recall: 0.8736\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4904 - precision: 0.8120 - recall: 0.7713 - val_loss: 0.3845 - val_precision: 0.4082 - val_recall: 0.8791\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4897 - precision: 0.8152 - recall: 0.7629 - val_loss: 0.4168 - val_precision: 0.3655 - val_recall: 0.8956\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.4858 - precision: 0.8144 - recall: 0.7634 - val_loss: 0.4461 - val_precision: 0.3367 - val_recall: 0.9231\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4879 - precision: 0.8154 - recall: 0.7613 - val_loss: 0.3896 - val_precision: 0.3839 - val_recall: 0.8901\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4745 - precision: 0.8201 - recall: 0.7717 - val_loss: 0.4025 - val_precision: 0.3727 - val_recall: 0.9011\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.4720 - precision: 0.8161 - recall: 0.7849 - val_loss: 0.3796 - val_precision: 0.3966 - val_recall: 0.8846\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.4689 - precision: 0.8227 - recall: 0.7681 - val_loss: 0.3929 - val_precision: 0.3767 - val_recall: 0.8901\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4731 - precision: 0.8243 - recall: 0.7830 - val_loss: 0.3735 - val_precision: 0.4000 - val_recall: 0.8791\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.4602 - precision: 0.8253 - recall: 0.7811 - val_loss: 0.3653 - val_precision: 0.4119 - val_recall: 0.8736\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.4636 - precision: 0.8211 - recall: 0.7806 - val_loss: 0.3406 - val_precision: 0.4361 - val_recall: 0.8626\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.4599 - precision: 0.8360 - recall: 0.7751 - val_loss: 0.3788 - val_precision: 0.3961 - val_recall: 0.8901\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4582 - precision: 0.8363 - recall: 0.7854 - val_loss: 0.3813 - val_precision: 0.3932 - val_recall: 0.8901\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4486 - precision: 0.8331 - recall: 0.7866 - val_loss: 0.3823 - val_precision: 0.3835 - val_recall: 0.8956\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4530 - precision: 0.8382 - recall: 0.7866 - val_loss: 0.3800 - val_precision: 0.3890 - val_recall: 0.8956\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4475 - precision: 0.8309 - recall: 0.7876 - val_loss: 0.3623 - val_precision: 0.4035 - val_recall: 0.8846\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4403 - precision: 0.8373 - recall: 0.7935 - val_loss: 0.3456 - val_precision: 0.4267 - val_recall: 0.8791\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4391 - precision: 0.8400 - recall: 0.7862 - val_loss: 0.3636 - val_precision: 0.4103 - val_recall: 0.8791\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.4298 - precision: 0.8400 - recall: 0.8087 - val_loss: 0.3552 - val_precision: 0.4107 - val_recall: 0.8846\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4357 - precision: 0.8410 - recall: 0.7923 - val_loss: 0.3309 - val_precision: 0.4348 - val_recall: 0.8791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efb0739ee90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOBrf9sFMMAo",
        "outputId": "fcc87e6e-22b2-46ab-a1ff-431871db6e93"
      },
      "source": [
        "epochs=100\n",
        "batch_size=512\n",
        "model.fit(input_train, y_train, epochs=epochs, batch_size=batch_size,  validation_data=(input_test, y_test))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.4296 - precision: 0.8517 - recall: 0.8047 - val_loss: 0.3510 - val_precision: 0.4118 - val_recall: 0.8846\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.4306 - precision: 0.8411 - recall: 0.8021 - val_loss: 0.3399 - val_precision: 0.4336 - val_recall: 0.8791\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.4277 - precision: 0.8427 - recall: 0.8097 - val_loss: 0.3487 - val_precision: 0.4171 - val_recall: 0.8846\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.4225 - precision: 0.8504 - recall: 0.8092 - val_loss: 0.3408 - val_precision: 0.4271 - val_recall: 0.8846\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.4189 - precision: 0.8517 - recall: 0.8097 - val_loss: 0.3353 - val_precision: 0.4313 - val_recall: 0.8791\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4182 - precision: 0.8597 - recall: 0.8035 - val_loss: 0.3649 - val_precision: 0.4070 - val_recall: 0.8901\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.4151 - precision: 0.8595 - recall: 0.8118 - val_loss: 0.3441 - val_precision: 0.4263 - val_recall: 0.8736\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.4065 - precision: 0.8578 - recall: 0.8147 - val_loss: 0.3381 - val_precision: 0.4255 - val_recall: 0.8791\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4073 - precision: 0.8602 - recall: 0.8145 - val_loss: 0.3289 - val_precision: 0.4344 - val_recall: 0.8736\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.4113 - precision: 0.8566 - recall: 0.8201 - val_loss: 0.3160 - val_precision: 0.4633 - val_recall: 0.8681\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.4008 - precision: 0.8601 - recall: 0.8211 - val_loss: 0.3184 - val_precision: 0.4569 - val_recall: 0.8736\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.4016 - precision: 0.8658 - recall: 0.8177 - val_loss: 0.3429 - val_precision: 0.4226 - val_recall: 0.8846\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.4039 - precision: 0.8645 - recall: 0.8243 - val_loss: 0.3380 - val_precision: 0.4313 - val_recall: 0.8791\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3923 - precision: 0.8637 - recall: 0.8284 - val_loss: 0.3332 - val_precision: 0.4332 - val_recall: 0.8736\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3896 - precision: 0.8636 - recall: 0.8297 - val_loss: 0.3467 - val_precision: 0.4255 - val_recall: 0.8791\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3910 - precision: 0.8626 - recall: 0.8375 - val_loss: 0.3396 - val_precision: 0.4309 - val_recall: 0.8736\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3875 - precision: 0.8643 - recall: 0.8255 - val_loss: 0.3188 - val_precision: 0.4606 - val_recall: 0.8681\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3898 - precision: 0.8678 - recall: 0.8235 - val_loss: 0.3418 - val_precision: 0.4278 - val_recall: 0.8791\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3809 - precision: 0.8630 - recall: 0.8404 - val_loss: 0.3219 - val_precision: 0.4476 - val_recall: 0.8681\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3909 - precision: 0.8668 - recall: 0.8297 - val_loss: 0.3439 - val_precision: 0.4244 - val_recall: 0.8791\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3749 - precision: 0.8615 - recall: 0.8416 - val_loss: 0.3296 - val_precision: 0.4510 - val_recall: 0.8846\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3742 - precision: 0.8736 - recall: 0.8351 - val_loss: 0.3299 - val_precision: 0.4522 - val_recall: 0.8846\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3674 - precision: 0.8747 - recall: 0.8475 - val_loss: 0.3194 - val_precision: 0.4580 - val_recall: 0.8681\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3807 - precision: 0.8748 - recall: 0.8268 - val_loss: 0.3427 - val_precision: 0.4387 - val_recall: 0.8846\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3686 - precision: 0.8743 - recall: 0.8385 - val_loss: 0.3311 - val_precision: 0.4472 - val_recall: 0.8846\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 7s 287ms/step - loss: 0.3707 - precision: 0.8651 - recall: 0.8389 - val_loss: 0.3180 - val_precision: 0.4636 - val_recall: 0.8736\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3735 - precision: 0.8704 - recall: 0.8400 - val_loss: 0.3216 - val_precision: 0.4611 - val_recall: 0.8791\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3620 - precision: 0.8792 - recall: 0.8402 - val_loss: 0.3581 - val_precision: 0.4237 - val_recall: 0.8846\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3618 - precision: 0.8789 - recall: 0.8478 - val_loss: 0.3527 - val_precision: 0.4267 - val_recall: 0.8791\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3645 - precision: 0.8773 - recall: 0.8377 - val_loss: 0.3135 - val_precision: 0.4675 - val_recall: 0.8681\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3569 - precision: 0.8753 - recall: 0.8532 - val_loss: 0.3208 - val_precision: 0.4609 - val_recall: 0.8736\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3506 - precision: 0.8809 - recall: 0.8526 - val_loss: 0.3217 - val_precision: 0.4543 - val_recall: 0.8736\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.3580 - precision: 0.8771 - recall: 0.8516 - val_loss: 0.3267 - val_precision: 0.4545 - val_recall: 0.8791\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3479 - precision: 0.8792 - recall: 0.8534 - val_loss: 0.3283 - val_precision: 0.4558 - val_recall: 0.8791\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3554 - precision: 0.8789 - recall: 0.8500 - val_loss: 0.3093 - val_precision: 0.4745 - val_recall: 0.8681\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3520 - precision: 0.8851 - recall: 0.8450 - val_loss: 0.3244 - val_precision: 0.4582 - val_recall: 0.8736\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3563 - precision: 0.8779 - recall: 0.8482 - val_loss: 0.3206 - val_precision: 0.4675 - val_recall: 0.8681\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.3430 - precision: 0.8886 - recall: 0.8453 - val_loss: 0.3379 - val_precision: 0.4556 - val_recall: 0.8736\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3372 - precision: 0.8883 - recall: 0.8439 - val_loss: 0.3331 - val_precision: 0.4571 - val_recall: 0.8791\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3384 - precision: 0.8912 - recall: 0.8551 - val_loss: 0.3345 - val_precision: 0.4611 - val_recall: 0.8791\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3428 - precision: 0.8852 - recall: 0.8614 - val_loss: 0.3297 - val_precision: 0.4663 - val_recall: 0.8736\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3373 - precision: 0.8917 - recall: 0.8555 - val_loss: 0.3452 - val_precision: 0.4482 - val_recall: 0.8791\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.3415 - precision: 0.8838 - recall: 0.8583 - val_loss: 0.3115 - val_precision: 0.4877 - val_recall: 0.8681\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3435 - precision: 0.8874 - recall: 0.8631 - val_loss: 0.2941 - val_precision: 0.4968 - val_recall: 0.8571\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 7s 279ms/step - loss: 0.3376 - precision: 0.8833 - recall: 0.8612 - val_loss: 0.2989 - val_precision: 0.4906 - val_recall: 0.8571\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 7s 279ms/step - loss: 0.3287 - precision: 0.8921 - recall: 0.8526 - val_loss: 0.3208 - val_precision: 0.4673 - val_recall: 0.8626\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3314 - precision: 0.8907 - recall: 0.8590 - val_loss: 0.2937 - val_precision: 0.4984 - val_recall: 0.8571\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3252 - precision: 0.8899 - recall: 0.8541 - val_loss: 0.3089 - val_precision: 0.4831 - val_recall: 0.8626\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3296 - precision: 0.8896 - recall: 0.8577 - val_loss: 0.3452 - val_precision: 0.4520 - val_recall: 0.8791\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3301 - precision: 0.8918 - recall: 0.8610 - val_loss: 0.3045 - val_precision: 0.4921 - val_recall: 0.8571\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3267 - precision: 0.8945 - recall: 0.8636 - val_loss: 0.3369 - val_precision: 0.4558 - val_recall: 0.8791\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3281 - precision: 0.8942 - recall: 0.8639 - val_loss: 0.3437 - val_precision: 0.4460 - val_recall: 0.8846\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3223 - precision: 0.8930 - recall: 0.8658 - val_loss: 0.3005 - val_precision: 0.4937 - val_recall: 0.8571\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3283 - precision: 0.8887 - recall: 0.8693 - val_loss: 0.2885 - val_precision: 0.5065 - val_recall: 0.8571\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3248 - precision: 0.8978 - recall: 0.8605 - val_loss: 0.3299 - val_precision: 0.4708 - val_recall: 0.8846\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.3168 - precision: 0.8977 - recall: 0.8661 - val_loss: 0.3372 - val_precision: 0.4587 - val_recall: 0.8846\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3161 - precision: 0.8950 - recall: 0.8712 - val_loss: 0.3036 - val_precision: 0.4968 - val_recall: 0.8626\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3111 - precision: 0.8957 - recall: 0.8768 - val_loss: 0.3244 - val_precision: 0.4820 - val_recall: 0.8846\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3135 - precision: 0.8928 - recall: 0.8741 - val_loss: 0.2848 - val_precision: 0.5235 - val_recall: 0.8571\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3160 - precision: 0.8920 - recall: 0.8707 - val_loss: 0.3025 - val_precision: 0.5016 - val_recall: 0.8626\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3123 - precision: 0.8979 - recall: 0.8720 - val_loss: 0.3073 - val_precision: 0.4921 - val_recall: 0.8571\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3068 - precision: 0.8932 - recall: 0.8722 - val_loss: 0.3190 - val_precision: 0.4758 - val_recall: 0.8626\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3038 - precision: 0.8996 - recall: 0.8703 - val_loss: 0.3291 - val_precision: 0.4777 - val_recall: 0.8846\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3123 - precision: 0.8998 - recall: 0.8644 - val_loss: 0.3114 - val_precision: 0.4816 - val_recall: 0.8626\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3113 - precision: 0.8910 - recall: 0.8734 - val_loss: 0.3106 - val_precision: 0.4832 - val_recall: 0.8681\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.2967 - precision: 0.9021 - recall: 0.8797 - val_loss: 0.3242 - val_precision: 0.4716 - val_recall: 0.8681\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.3036 - precision: 0.9041 - recall: 0.8712 - val_loss: 0.3207 - val_precision: 0.4787 - val_recall: 0.8626\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3035 - precision: 0.8970 - recall: 0.8803 - val_loss: 0.3188 - val_precision: 0.4832 - val_recall: 0.8681\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.3037 - precision: 0.8950 - recall: 0.8788 - val_loss: 0.2856 - val_precision: 0.5324 - val_recall: 0.8571\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.3008 - precision: 0.9046 - recall: 0.8712 - val_loss: 0.3396 - val_precision: 0.4709 - val_recall: 0.8901\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.2953 - precision: 0.9014 - recall: 0.8835 - val_loss: 0.3058 - val_precision: 0.5000 - val_recall: 0.8626\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.3036 - precision: 0.9004 - recall: 0.8739 - val_loss: 0.2885 - val_precision: 0.5270 - val_recall: 0.8571\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.2938 - precision: 0.9034 - recall: 0.8790 - val_loss: 0.3304 - val_precision: 0.4732 - val_recall: 0.8736\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.2959 - precision: 0.9045 - recall: 0.8719 - val_loss: 0.3040 - val_precision: 0.5032 - val_recall: 0.8626\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.2999 - precision: 0.8998 - recall: 0.8725 - val_loss: 0.3417 - val_precision: 0.4721 - val_recall: 0.8846\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2907 - precision: 0.9022 - recall: 0.8763 - val_loss: 0.3103 - val_precision: 0.5000 - val_recall: 0.8736\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2899 - precision: 0.8992 - recall: 0.8876 - val_loss: 0.3191 - val_precision: 0.4893 - val_recall: 0.8791\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.2801 - precision: 0.9036 - recall: 0.8802 - val_loss: 0.3130 - val_precision: 0.4938 - val_recall: 0.8736\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.2818 - precision: 0.9098 - recall: 0.8876 - val_loss: 0.3119 - val_precision: 0.4923 - val_recall: 0.8736\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.2923 - precision: 0.8981 - recall: 0.8820 - val_loss: 0.3156 - val_precision: 0.4877 - val_recall: 0.8736\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2925 - precision: 0.9041 - recall: 0.8781 - val_loss: 0.3195 - val_precision: 0.4818 - val_recall: 0.8736\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2899 - precision: 0.9053 - recall: 0.8839 - val_loss: 0.3030 - val_precision: 0.4969 - val_recall: 0.8681\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2776 - precision: 0.9066 - recall: 0.8957 - val_loss: 0.3106 - val_precision: 0.4938 - val_recall: 0.8736\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 7s 287ms/step - loss: 0.2800 - precision: 0.9132 - recall: 0.8852 - val_loss: 0.3281 - val_precision: 0.4863 - val_recall: 0.8791\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 7s 294ms/step - loss: 0.2850 - precision: 0.9099 - recall: 0.8802 - val_loss: 0.3184 - val_precision: 0.4923 - val_recall: 0.8791\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 7s 291ms/step - loss: 0.2841 - precision: 0.9062 - recall: 0.8861 - val_loss: 0.3123 - val_precision: 0.5048 - val_recall: 0.8736\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 7s 288ms/step - loss: 0.2796 - precision: 0.9082 - recall: 0.8922 - val_loss: 0.2876 - val_precision: 0.5379 - val_recall: 0.8571\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 7s 288ms/step - loss: 0.2839 - precision: 0.9063 - recall: 0.8844 - val_loss: 0.3019 - val_precision: 0.5233 - val_recall: 0.8626\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 7s 286ms/step - loss: 0.2765 - precision: 0.9064 - recall: 0.8885 - val_loss: 0.3187 - val_precision: 0.4984 - val_recall: 0.8791\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 7s 287ms/step - loss: 0.2771 - precision: 0.9077 - recall: 0.8873 - val_loss: 0.3303 - val_precision: 0.4893 - val_recall: 0.8791\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 7s 280ms/step - loss: 0.2749 - precision: 0.9069 - recall: 0.8900 - val_loss: 0.3005 - val_precision: 0.5233 - val_recall: 0.8626\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.2684 - precision: 0.9089 - recall: 0.8937 - val_loss: 0.3111 - val_precision: 0.5064 - val_recall: 0.8736\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 7s 282ms/step - loss: 0.2863 - precision: 0.9123 - recall: 0.8820 - val_loss: 0.3145 - val_precision: 0.5016 - val_recall: 0.8791\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2758 - precision: 0.9061 - recall: 0.8807 - val_loss: 0.3166 - val_precision: 0.5000 - val_recall: 0.8791\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 7s 284ms/step - loss: 0.2770 - precision: 0.9095 - recall: 0.8898 - val_loss: 0.3227 - val_precision: 0.4984 - val_recall: 0.8791\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2764 - precision: 0.9100 - recall: 0.8866 - val_loss: 0.3027 - val_precision: 0.5163 - val_recall: 0.8681\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.2658 - precision: 0.9123 - recall: 0.8908 - val_loss: 0.3161 - val_precision: 0.5064 - val_recall: 0.8681\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2665 - precision: 0.9157 - recall: 0.8839 - val_loss: 0.3234 - val_precision: 0.5000 - val_recall: 0.8736\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 7s 283ms/step - loss: 0.2749 - precision: 0.9069 - recall: 0.8857 - val_loss: 0.3079 - val_precision: 0.5113 - val_recall: 0.8681\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.2723 - precision: 0.9082 - recall: 0.8871 - val_loss: 0.3136 - val_precision: 0.5096 - val_recall: 0.8736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efb07379150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFwMKbWRQrMn",
        "outputId": "a0f813d2-816c-4a29-ebd3-99aa3fd92496"
      },
      "source": [
        "filename='/content/drive/MyDrive/data_challenge/lstm_model.tf'\n",
        "model.save(filename)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_challenge/lstm_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_challenge/lstm_model.tf/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORYwXN1wI9Yn"
      },
      "source": [
        "import io\n",
        "import json\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with io.open('/content/drive/MyDrive/data_challenge/tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_1mCyBpu9xv"
      },
      "source": [
        "model=tf.keras.models.load_model('/content/drive/MyDrive/data_challenge/lstm_model.tf')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St_2DQbK2vlN"
      },
      "source": [
        "prediction=pd.Series(model.predict(input_test).reshape(-1,))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b1xsGy922TX"
      },
      "source": [
        "metrics=pd.DataFrame()\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDferfga3SP6"
      },
      "source": [
        "metrics['threshold']=np.arange(0,1,0.01)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXmvvBIA3ZAA"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd7RY5Cq3c6l"
      },
      "source": [
        "def threshold_prediction(threshold, prediction):\n",
        "  pred=1 if prediction >= threshold else 0\n",
        "  return pred"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEAsrIrd4TZX"
      },
      "source": [
        "for row in metrics.index:\n",
        "  metrics.at[row, 'precision']=precision_score(y_test,prediction.map(lambda x: threshold_prediction(metrics.at[row, 'threshold'], x)))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSDlrkNc5yo5"
      },
      "source": [
        "for row in metrics.index:\n",
        "  metrics.at[row, 'recall']=recall_score(y_test,prediction.map(lambda x: threshold_prediction(metrics.at[row, 'threshold'], x)))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXk97S8b6lFY"
      },
      "source": [
        "for row in metrics.index:\n",
        "  metrics.at[row, 'f1']=f1_score(y_test,prediction.map(lambda x: threshold_prediction(metrics.at[row, 'threshold'], x)))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nyIjGFuF6GZf",
        "outputId": "7a8de36b-142e-47e1-a496-4243678fd194"
      },
      "source": [
        "pd.options.display.max_rows = 102\n",
        "metrics\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threshold</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.109771</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.197826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.216482</td>\n",
              "      <td>0.967033</td>\n",
              "      <td>0.353769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.234501</td>\n",
              "      <td>0.956044</td>\n",
              "      <td>0.376623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.246439</td>\n",
              "      <td>0.950549</td>\n",
              "      <td>0.391403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.258209</td>\n",
              "      <td>0.950549</td>\n",
              "      <td>0.406103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.265842</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.414958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.276083</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.427329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.285240</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.438217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.291032</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.445019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.09</td>\n",
              "      <td>0.301226</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.456839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.311594</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.468665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.314917</td>\n",
              "      <td>0.939560</td>\n",
              "      <td>0.471724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.322642</td>\n",
              "      <td>0.939560</td>\n",
              "      <td>0.480337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.326255</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.482857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.331373</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.488439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.333996</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.490511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.342159</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.499257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.349272</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.506787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.18</td>\n",
              "      <td>0.357602</td>\n",
              "      <td>0.917582</td>\n",
              "      <td>0.514638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.367257</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.523659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.379863</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.536349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.385151</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.541599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.394299</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.550580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.556114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.405868</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.561760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.411911</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.567521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.418136</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.573402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.27</td>\n",
              "      <td>0.430052</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.584507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.434555</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.588652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.440318</td>\n",
              "      <td>0.912088</td>\n",
              "      <td>0.593918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.449591</td>\n",
              "      <td>0.906593</td>\n",
              "      <td>0.601093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.906593</td>\n",
              "      <td>0.605505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.452778</td>\n",
              "      <td>0.895604</td>\n",
              "      <td>0.601476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.459155</td>\n",
              "      <td>0.895604</td>\n",
              "      <td>0.607076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.607880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.466859</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.612476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.466859</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.612476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.470930</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.615970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.475073</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.619503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.39</td>\n",
              "      <td>0.477745</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.620424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.483384</td>\n",
              "      <td>0.879121</td>\n",
              "      <td>0.623782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.41</td>\n",
              "      <td>0.484756</td>\n",
              "      <td>0.873626</td>\n",
              "      <td>0.623529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.487730</td>\n",
              "      <td>0.873626</td>\n",
              "      <td>0.625984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.43</td>\n",
              "      <td>0.492260</td>\n",
              "      <td>0.873626</td>\n",
              "      <td>0.629703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.493789</td>\n",
              "      <td>0.873626</td>\n",
              "      <td>0.630952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.498433</td>\n",
              "      <td>0.873626</td>\n",
              "      <td>0.634731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.498423</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.633267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.501587</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.635815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.638384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.509677</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.642276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.512987</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.644898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.512987</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.644898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.512987</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.644898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.516340</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.647541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.521452</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.651546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.530201</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.658333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.530405</td>\n",
              "      <td>0.862637</td>\n",
              "      <td>0.656904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0.528814</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.654088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.539792</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.662420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.59</td>\n",
              "      <td>0.543554</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.665245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.553191</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.672414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.61</td>\n",
              "      <td>0.557143</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.675325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.62</td>\n",
              "      <td>0.561594</td>\n",
              "      <td>0.851648</td>\n",
              "      <td>0.676856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.565693</td>\n",
              "      <td>0.851648</td>\n",
              "      <td>0.679825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.64</td>\n",
              "      <td>0.566176</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.678414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0.573034</td>\n",
              "      <td>0.840659</td>\n",
              "      <td>0.681514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.577358</td>\n",
              "      <td>0.840659</td>\n",
              "      <td>0.684564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.579545</td>\n",
              "      <td>0.840659</td>\n",
              "      <td>0.686099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.584615</td>\n",
              "      <td>0.835165</td>\n",
              "      <td>0.687783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.583658</td>\n",
              "      <td>0.824176</td>\n",
              "      <td>0.683371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.593625</td>\n",
              "      <td>0.818681</td>\n",
              "      <td>0.688222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.71</td>\n",
              "      <td>0.594378</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.686775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.72</td>\n",
              "      <td>0.599190</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.689977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.601626</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.691589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.609053</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.696471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.614108</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.699764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.612500</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.696682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.615063</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.698337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0.620253</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.701671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.79</td>\n",
              "      <td>0.622881</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.703349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.628205</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.706731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.629310</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.705314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.629310</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.705314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.83</td>\n",
              "      <td>0.638767</td>\n",
              "      <td>0.796703</td>\n",
              "      <td>0.709046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.641593</td>\n",
              "      <td>0.796703</td>\n",
              "      <td>0.710784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.709360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.651584</td>\n",
              "      <td>0.791209</td>\n",
              "      <td>0.714640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.711443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.663551</td>\n",
              "      <td>0.780220</td>\n",
              "      <td>0.717172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.668246</td>\n",
              "      <td>0.774725</td>\n",
              "      <td>0.717557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.676329</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.719794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.688119</td>\n",
              "      <td>0.763736</td>\n",
              "      <td>0.723958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.695431</td>\n",
              "      <td>0.752747</td>\n",
              "      <td>0.722955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.701571</td>\n",
              "      <td>0.736264</td>\n",
              "      <td>0.718499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.94</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.736264</td>\n",
              "      <td>0.728261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.95</td>\n",
              "      <td>0.725275</td>\n",
              "      <td>0.725275</td>\n",
              "      <td>0.725275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.703297</td>\n",
              "      <td>0.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.97</td>\n",
              "      <td>0.768293</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.728324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.98</td>\n",
              "      <td>0.802632</td>\n",
              "      <td>0.670330</td>\n",
              "      <td>0.730539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.99</td>\n",
              "      <td>0.847458</td>\n",
              "      <td>0.549451</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    threshold  precision    recall        f1\n",
              "0        0.00   0.109771  1.000000  0.197826\n",
              "1        0.01   0.216482  0.967033  0.353769\n",
              "2        0.02   0.234501  0.956044  0.376623\n",
              "3        0.03   0.246439  0.950549  0.391403\n",
              "4        0.04   0.258209  0.950549  0.406103\n",
              "5        0.05   0.265842  0.945055  0.414958\n",
              "6        0.06   0.276083  0.945055  0.427329\n",
              "7        0.07   0.285240  0.945055  0.438217\n",
              "8        0.08   0.291032  0.945055  0.445019\n",
              "9        0.09   0.301226  0.945055  0.456839\n",
              "10       0.10   0.311594  0.945055  0.468665\n",
              "11       0.11   0.314917  0.939560  0.471724\n",
              "12       0.12   0.322642  0.939560  0.480337\n",
              "13       0.13   0.326255  0.928571  0.482857\n",
              "14       0.14   0.331373  0.928571  0.488439\n",
              "15       0.15   0.333996  0.923077  0.490511\n",
              "16       0.16   0.342159  0.923077  0.499257\n",
              "17       0.17   0.349272  0.923077  0.506787\n",
              "18       0.18   0.357602  0.917582  0.514638\n",
              "19       0.19   0.367257  0.912088  0.523659\n",
              "20       0.20   0.379863  0.912088  0.536349\n",
              "21       0.21   0.385151  0.912088  0.541599\n",
              "22       0.22   0.394299  0.912088  0.550580\n",
              "23       0.23   0.400000  0.912088  0.556114\n",
              "24       0.24   0.405868  0.912088  0.561760\n",
              "25       0.25   0.411911  0.912088  0.567521\n",
              "26       0.26   0.418136  0.912088  0.573402\n",
              "27       0.27   0.430052  0.912088  0.584507\n",
              "28       0.28   0.434555  0.912088  0.588652\n",
              "29       0.29   0.440318  0.912088  0.593918\n",
              "30       0.30   0.449591  0.906593  0.601093\n",
              "31       0.31   0.454545  0.906593  0.605505\n",
              "32       0.32   0.452778  0.895604  0.601476\n",
              "33       0.33   0.459155  0.895604  0.607076\n",
              "34       0.34   0.461538  0.890110  0.607880\n",
              "35       0.35   0.466859  0.890110  0.612476\n",
              "36       0.36   0.466859  0.890110  0.612476\n",
              "37       0.37   0.470930  0.890110  0.615970\n",
              "38       0.38   0.475073  0.890110  0.619503\n",
              "39       0.39   0.477745  0.884615  0.620424\n",
              "40       0.40   0.483384  0.879121  0.623782\n",
              "41       0.41   0.484756  0.873626  0.623529\n",
              "42       0.42   0.487730  0.873626  0.625984\n",
              "43       0.43   0.492260  0.873626  0.629703\n",
              "44       0.44   0.493789  0.873626  0.630952\n",
              "45       0.45   0.498433  0.873626  0.634731\n",
              "46       0.46   0.498423  0.868132  0.633267\n",
              "47       0.47   0.501587  0.868132  0.635815\n",
              "48       0.48   0.504792  0.868132  0.638384\n",
              "49       0.49   0.509677  0.868132  0.642276\n",
              "50       0.50   0.512987  0.868132  0.644898\n",
              "51       0.51   0.512987  0.868132  0.644898\n",
              "52       0.52   0.512987  0.868132  0.644898\n",
              "53       0.53   0.516340  0.868132  0.647541\n",
              "54       0.54   0.521452  0.868132  0.651546\n",
              "55       0.55   0.530201  0.868132  0.658333\n",
              "56       0.56   0.530405  0.862637  0.656904\n",
              "57       0.57   0.528814  0.857143  0.654088\n",
              "58       0.58   0.539792  0.857143  0.662420\n",
              "59       0.59   0.543554  0.857143  0.665245\n",
              "60       0.60   0.553191  0.857143  0.672414\n",
              "61       0.61   0.557143  0.857143  0.675325\n",
              "62       0.62   0.561594  0.851648  0.676856\n",
              "63       0.63   0.565693  0.851648  0.679825\n",
              "64       0.64   0.566176  0.846154  0.678414\n",
              "65       0.65   0.573034  0.840659  0.681514\n",
              "66       0.66   0.577358  0.840659  0.684564\n",
              "67       0.67   0.579545  0.840659  0.686099\n",
              "68       0.68   0.584615  0.835165  0.687783\n",
              "69       0.69   0.583658  0.824176  0.683371\n",
              "70       0.70   0.593625  0.818681  0.688222\n",
              "71       0.71   0.594378  0.813187  0.686775\n",
              "72       0.72   0.599190  0.813187  0.689977\n",
              "73       0.73   0.601626  0.813187  0.691589\n",
              "74       0.74   0.609053  0.813187  0.696471\n",
              "75       0.75   0.614108  0.813187  0.699764\n",
              "76       0.76   0.612500  0.807692  0.696682\n",
              "77       0.77   0.615063  0.807692  0.698337\n",
              "78       0.78   0.620253  0.807692  0.701671\n",
              "79       0.79   0.622881  0.807692  0.703349\n",
              "80       0.80   0.628205  0.807692  0.706731\n",
              "81       0.81   0.629310  0.802198  0.705314\n",
              "82       0.82   0.629310  0.802198  0.705314\n",
              "83       0.83   0.638767  0.796703  0.709046\n",
              "84       0.84   0.641593  0.796703  0.710784\n",
              "85       0.85   0.642857  0.791209  0.709360\n",
              "86       0.86   0.651584  0.791209  0.714640\n",
              "87       0.87   0.650000  0.785714  0.711443\n",
              "88       0.88   0.663551  0.780220  0.717172\n",
              "89       0.89   0.668246  0.774725  0.717557\n",
              "90       0.90   0.676329  0.769231  0.719794\n",
              "91       0.91   0.688119  0.763736  0.723958\n",
              "92       0.92   0.695431  0.752747  0.722955\n",
              "93       0.93   0.701571  0.736264  0.718499\n",
              "94       0.94   0.720430  0.736264  0.728261\n",
              "95       0.95   0.725275  0.725275  0.725275\n",
              "96       0.96   0.752941  0.703297  0.727273\n",
              "97       0.97   0.768293  0.692308  0.728324\n",
              "98       0.98   0.802632  0.670330  0.730539\n",
              "99       0.99   0.847458  0.549451  0.666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZDLDZtf6TZt"
      },
      "source": [
        "#Check performance with the Validation Data"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dumnK3RHBV4"
      },
      "source": [
        "input_val = prepare_inputs(val_data.sentence1, max_len)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwqg7FErHG1E"
      },
      "source": [
        "val_prediction=pd.Series(model.predict(input_val).reshape(-1,))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p2NhYMNHIqs"
      },
      "source": [
        "val_metrics=pd.DataFrame()\n",
        "val_metrics['threshold']=np.arange(0,1,0.01)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFm6KKz1HomO"
      },
      "source": [
        "for row in val_metrics.index:\n",
        "  val_metrics.at[row, 'precision']=precision_score(val_data.label,val_prediction.map(lambda x: threshold_prediction(val_metrics.at[row, 'threshold'], x)))\n",
        "  val_metrics.at[row, 'recall']=recall_score(val_data.label,val_prediction.map(lambda x: threshold_prediction(val_metrics.at[row, 'threshold'], x)))\n",
        "  val_metrics.at[row, 'f1']=f1_score(val_data.label,val_prediction.map(lambda x: threshold_prediction(val_metrics.at[row, 'threshold'], x)))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W0F9tTG4H3pt",
        "outputId": "fd48c8d4-229e-49ba-d1dc-4a4db13976ef"
      },
      "source": [
        "val_metrics"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threshold</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.117137</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.209709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.234649</td>\n",
              "      <td>0.990741</td>\n",
              "      <td>0.379433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.251765</td>\n",
              "      <td>0.990741</td>\n",
              "      <td>0.401501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.258621</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.408560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.426829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.438413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.289256</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.445860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.458515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.310651</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.470852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.09</td>\n",
              "      <td>0.317221</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.478360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>0.480370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>0.485981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.331190</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.491647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.345638</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.507389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.355172</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.517588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.361404</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.524173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.363958</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.526854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.374545</td>\n",
              "      <td>0.953704</td>\n",
              "      <td>0.537859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.18</td>\n",
              "      <td>0.376384</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.538259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.380597</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.542553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.384906</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.546917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.389313</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.551351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.398438</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.560440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.403162</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.565097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.400794</td>\n",
              "      <td>0.935185</td>\n",
              "      <td>0.561111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.405622</td>\n",
              "      <td>0.935185</td>\n",
              "      <td>0.565826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.413934</td>\n",
              "      <td>0.935185</td>\n",
              "      <td>0.573864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.27</td>\n",
              "      <td>0.414938</td>\n",
              "      <td>0.925926</td>\n",
              "      <td>0.573066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.417722</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.573913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.423077</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.578947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.424893</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.580645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.430435</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.585799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.436123</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.591045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.594595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.443946</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.598187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.452055</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.605505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.460465</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.613003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.462617</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.614907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.39</td>\n",
              "      <td>0.460094</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.610592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.464455</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.614420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.41</td>\n",
              "      <td>0.464455</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.614420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.471154</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.620253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.43</td>\n",
              "      <td>0.470874</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.617834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.473171</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.619808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.480198</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.625806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.480198</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.625806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.480198</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.625806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.480198</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.625806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.480198</td>\n",
              "      <td>0.898148</td>\n",
              "      <td>0.625806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.482412</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.625407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.627451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.631579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.631579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.631579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.487179</td>\n",
              "      <td>0.879630</td>\n",
              "      <td>0.627063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.487179</td>\n",
              "      <td>0.879630</td>\n",
              "      <td>0.627063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0.487047</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.624585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.492147</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.628763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.59</td>\n",
              "      <td>0.494681</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.628378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.497326</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.630508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.61</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.632653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.62</td>\n",
              "      <td>0.508197</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.639175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.645833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.64</td>\n",
              "      <td>0.522472</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.650350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0.522472</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.650350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.534483</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.659574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.535294</td>\n",
              "      <td>0.842593</td>\n",
              "      <td>0.654676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.544910</td>\n",
              "      <td>0.842593</td>\n",
              "      <td>0.661818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.548193</td>\n",
              "      <td>0.842593</td>\n",
              "      <td>0.664234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.554878</td>\n",
              "      <td>0.842593</td>\n",
              "      <td>0.669118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.71</td>\n",
              "      <td>0.554878</td>\n",
              "      <td>0.842593</td>\n",
              "      <td>0.669118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.72</td>\n",
              "      <td>0.543750</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.649254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.550633</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.654135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.561290</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.661597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.568627</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.674419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.595890</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.685039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0.595890</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.685039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.79</td>\n",
              "      <td>0.608392</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.693227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.608392</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.693227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.605634</td>\n",
              "      <td>0.796296</td>\n",
              "      <td>0.688000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.615942</td>\n",
              "      <td>0.787037</td>\n",
              "      <td>0.691057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.83</td>\n",
              "      <td>0.634328</td>\n",
              "      <td>0.787037</td>\n",
              "      <td>0.702479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.639098</td>\n",
              "      <td>0.787037</td>\n",
              "      <td>0.705394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.648855</td>\n",
              "      <td>0.787037</td>\n",
              "      <td>0.711297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.643411</td>\n",
              "      <td>0.768519</td>\n",
              "      <td>0.700422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.645669</td>\n",
              "      <td>0.759259</td>\n",
              "      <td>0.697872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.650794</td>\n",
              "      <td>0.759259</td>\n",
              "      <td>0.700855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.658537</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.701299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.701754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.701754</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.720721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.690909</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.697248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.94</td>\n",
              "      <td>0.697248</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.700461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.95</td>\n",
              "      <td>0.721154</td>\n",
              "      <td>0.694444</td>\n",
              "      <td>0.707547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.725490</td>\n",
              "      <td>0.685185</td>\n",
              "      <td>0.704762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.97</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.648148</td>\n",
              "      <td>0.689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.98</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.574074</td>\n",
              "      <td>0.659574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.99</td>\n",
              "      <td>0.803571</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.548780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    threshold  precision    recall        f1\n",
              "0        0.00   0.117137  1.000000  0.209709\n",
              "1        0.01   0.234649  0.990741  0.379433\n",
              "2        0.02   0.251765  0.990741  0.401501\n",
              "3        0.03   0.258621  0.972222  0.408560\n",
              "4        0.04   0.273438  0.972222  0.426829\n",
              "5        0.05   0.283019  0.972222  0.438413\n",
              "6        0.06   0.289256  0.972222  0.445860\n",
              "7        0.07   0.300000  0.972222  0.458515\n",
              "8        0.08   0.310651  0.972222  0.470852\n",
              "9        0.09   0.317221  0.972222  0.478360\n",
              "10       0.10   0.320000  0.962963  0.480370\n",
              "11       0.11   0.325000  0.962963  0.485981\n",
              "12       0.12   0.331190  0.953704  0.491647\n",
              "13       0.13   0.345638  0.953704  0.507389\n",
              "14       0.14   0.355172  0.953704  0.517588\n",
              "15       0.15   0.361404  0.953704  0.524173\n",
              "16       0.16   0.363958  0.953704  0.526854\n",
              "17       0.17   0.374545  0.953704  0.537859\n",
              "18       0.18   0.376384  0.944444  0.538259\n",
              "19       0.19   0.380597  0.944444  0.542553\n",
              "20       0.20   0.384906  0.944444  0.546917\n",
              "21       0.21   0.389313  0.944444  0.551351\n",
              "22       0.22   0.398438  0.944444  0.560440\n",
              "23       0.23   0.403162  0.944444  0.565097\n",
              "24       0.24   0.400794  0.935185  0.561111\n",
              "25       0.25   0.405622  0.935185  0.565826\n",
              "26       0.26   0.413934  0.935185  0.573864\n",
              "27       0.27   0.414938  0.925926  0.573066\n",
              "28       0.28   0.417722  0.916667  0.573913\n",
              "29       0.29   0.423077  0.916667  0.578947\n",
              "30       0.30   0.424893  0.916667  0.580645\n",
              "31       0.31   0.430435  0.916667  0.585799\n",
              "32       0.32   0.436123  0.916667  0.591045\n",
              "33       0.33   0.440000  0.916667  0.594595\n",
              "34       0.34   0.443946  0.916667  0.598187\n",
              "35       0.35   0.452055  0.916667  0.605505\n",
              "36       0.36   0.458333  0.916667  0.611111\n",
              "37       0.37   0.460465  0.916667  0.613003\n",
              "38       0.38   0.462617  0.916667  0.614907\n",
              "39       0.39   0.460094  0.907407  0.610592\n",
              "40       0.40   0.464455  0.907407  0.614420\n",
              "41       0.41   0.464455  0.907407  0.614420\n",
              "42       0.42   0.471154  0.907407  0.620253\n",
              "43       0.43   0.470874  0.898148  0.617834\n",
              "44       0.44   0.473171  0.898148  0.619808\n",
              "45       0.45   0.480198  0.898148  0.625806\n",
              "46       0.46   0.480198  0.898148  0.625806\n",
              "47       0.47   0.480198  0.898148  0.625806\n",
              "48       0.48   0.480198  0.898148  0.625806\n",
              "49       0.49   0.480198  0.898148  0.625806\n",
              "50       0.50   0.482412  0.888889  0.625407\n",
              "51       0.51   0.484848  0.888889  0.627451\n",
              "52       0.52   0.489796  0.888889  0.631579\n",
              "53       0.53   0.489796  0.888889  0.631579\n",
              "54       0.54   0.489796  0.888889  0.631579\n",
              "55       0.55   0.487179  0.879630  0.627063\n",
              "56       0.56   0.487179  0.879630  0.627063\n",
              "57       0.57   0.487047  0.870370  0.624585\n",
              "58       0.58   0.492147  0.870370  0.628763\n",
              "59       0.59   0.494681  0.861111  0.628378\n",
              "60       0.60   0.497326  0.861111  0.630508\n",
              "61       0.61   0.500000  0.861111  0.632653\n",
              "62       0.62   0.508197  0.861111  0.639175\n",
              "63       0.63   0.516667  0.861111  0.645833\n",
              "64       0.64   0.522472  0.861111  0.650350\n",
              "65       0.65   0.522472  0.861111  0.650350\n",
              "66       0.66   0.534483  0.861111  0.659574\n",
              "67       0.67   0.535294  0.842593  0.654676\n",
              "68       0.68   0.544910  0.842593  0.661818\n",
              "69       0.69   0.548193  0.842593  0.664234\n",
              "70       0.70   0.554878  0.842593  0.669118\n",
              "71       0.71   0.554878  0.842593  0.669118\n",
              "72       0.72   0.543750  0.805556  0.649254\n",
              "73       0.73   0.550633  0.805556  0.654135\n",
              "74       0.74   0.561290  0.805556  0.661597\n",
              "75       0.75   0.568627  0.805556  0.666667\n",
              "76       0.76   0.580000  0.805556  0.674419\n",
              "77       0.77   0.595890  0.805556  0.685039\n",
              "78       0.78   0.595890  0.805556  0.685039\n",
              "79       0.79   0.608392  0.805556  0.693227\n",
              "80       0.80   0.608392  0.805556  0.693227\n",
              "81       0.81   0.605634  0.796296  0.688000\n",
              "82       0.82   0.615942  0.787037  0.691057\n",
              "83       0.83   0.634328  0.787037  0.702479\n",
              "84       0.84   0.639098  0.787037  0.705394\n",
              "85       0.85   0.648855  0.787037  0.711297\n",
              "86       0.86   0.643411  0.768519  0.700422\n",
              "87       0.87   0.645669  0.759259  0.697872\n",
              "88       0.88   0.650794  0.759259  0.700855\n",
              "89       0.89   0.658537  0.750000  0.701299\n",
              "90       0.90   0.666667  0.740741  0.701754\n",
              "91       0.91   0.689655  0.740741  0.714286\n",
              "92       0.92   0.701754  0.740741  0.720721\n",
              "93       0.93   0.690909  0.703704  0.697248\n",
              "94       0.94   0.697248  0.703704  0.700461\n",
              "95       0.95   0.721154  0.694444  0.707547\n",
              "96       0.96   0.725490  0.685185  0.704762\n",
              "97       0.97   0.736842  0.648148  0.689655\n",
              "98       0.98   0.775000  0.574074  0.659574\n",
              "99       0.99   0.803571  0.416667  0.548780"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_51u67o7IXul"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}